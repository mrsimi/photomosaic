{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import uuid\n",
    "\n",
    "\n",
    "def color_match(image1, image2):\n",
    "    img1 = cv2.imread(image1)\n",
    "    img2 = cv2.imread(image2)\n",
    "    img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))\n",
    "    \n",
    "    img1_lab = cv2.cvtColor(img1, cv2.COLOR_BGR2LAB)\n",
    "    img2_lab = cv2.cvtColor(img2, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Calculate mean and standard deviation for each channel in LAB color space\n",
    "    img1_mean, img1_std = cv2.meanStdDev(img1_lab)\n",
    "    img2_mean, img2_std = cv2.meanStdDev(img2_lab)\n",
    "\n",
    "    # Perform color matching by adjusting mean and standard deviation\n",
    "    for i in range(3):  # Iterate over L, A, B channels\n",
    "        img1_lab[:,:,i] = img1_lab[:,:,i] - img1_mean[i]\n",
    "        img1_lab[:,:,i] = img1_lab[:,:,i] * (img2_std[i] / img1_std[i])\n",
    "        img1_lab[:,:,i] = img1_lab[:,:,i] + img2_mean[i]\n",
    "\n",
    "    # Convert back to BGR color space\n",
    "    matched_img = cv2.cvtColor(img1_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return matched_img\n",
    "\n",
    "# Example usage\n",
    "#image1- targeImage\n",
    "#image2 - palletImage\n",
    "# target_image_path ='targetimage.jpeg'\n",
    "# pallet_image_path = 'palletimage.jpeg'\n",
    "# matched_image = color_match(target_image_path, pallet_image_path)\n",
    "\n",
    "# # Display the matched image\n",
    "# # cv2.imshow('Color Matched Image', matched_image)\n",
    "# # cv2.waitKey(0)\n",
    "# # cv2.destroyAllWindows()\n",
    "# output_path = str(uuid.uuid4())+'_matched_image.jpg'\n",
    "# cv2.imwrite(output_path, matched_image)\n",
    "# print(f\"Color matched image saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@22.160] global loadsave.cpp:248 findDecoder imread_('targetimage.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m num_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Number of rows for splitting\u001b[39;00m\n\u001b[1;32m      8\u001b[0m num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Number of columns for splitting\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m height, width, _ \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate the dimensions of each part\u001b[39;00m\n\u001b[1;32m     12\u001b[0m part_height \u001b[38;5;241m=\u001b[39m height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_rows\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('targetimage.jpeg')\n",
    "\n",
    "# Define the dimensions of each part\n",
    "num_rows = 3  # Number of rows for splitting\n",
    "num_cols = 3  # Number of columns for splitting\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Calculate the dimensions of each part\n",
    "part_height = height // num_rows\n",
    "part_width = width // num_cols\n",
    "\n",
    "# Initialize an empty list to store the parts\n",
    "image_parts = []\n",
    "\n",
    "# Iterate through rows and columns to split the image\n",
    "for row in range(num_rows):\n",
    "    for col in range(num_cols):\n",
    "        # Calculate the starting and ending coordinates for each part\n",
    "        start_row = row * part_height\n",
    "        end_row = (row + 1) * part_height\n",
    "        start_col = col * part_width\n",
    "        end_col = (col + 1) * part_width\n",
    "        \n",
    "        # Extract the part from the image\n",
    "        part = image[start_row:end_row, start_col:end_col]\n",
    "        \n",
    "        # Append the part to the list\n",
    "        image_parts.append(part)\n",
    "        \n",
    "for i, part in enumerate(image_parts):\n",
    "    cv2.imwrite(f'part_{i+1}.jpg', part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load all the image parts\n",
    "image_parts = []\n",
    "for i in range(1, num_rows*num_cols + 1):  # Assuming num_rows and num_cols are defined\n",
    "    #part = cv2.imread(f'part_{i}.jpg')\n",
    "    part = color_match(f'part_{i}.jpg', 'palletimage.jpeg')\n",
    "    image_parts.append(part)\n",
    "\n",
    "# Determine the dimensions of the combined image\n",
    "combined_height = num_rows * part_height\n",
    "combined_width = num_cols * part_width\n",
    "\n",
    "# Create an empty canvas for the combined image\n",
    "combined_image = np.zeros((combined_height, combined_width, 3), dtype=np.uint8)\n",
    "\n",
    "# Combine the image parts into the final image\n",
    "for i, part in enumerate(image_parts):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    start_row = row * part_height\n",
    "    end_row = (row + 1) * part_height\n",
    "    start_col = col * part_width\n",
    "    end_col = (col + 1) * part_width\n",
    "    combined_image[start_row:end_row, start_col:end_col] = part\n",
    "\n",
    "# Optionally, save the combined image to a file\n",
    "cv2.imwrite('combined_image.jpg', combined_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color matched image saved to 2298fc3a-1a36-49a2-ae75-bd85a363fa7a_matched_image.jpg\n"
     ]
    }
   ],
   "source": [
    "mth_jpg = color_match('part_1.jpg', 'palletimage.jpeg')\n",
    "output_path = str(uuid.uuid4())+'_matched_image.jpg'\n",
    "cv2.imwrite(output_path, mth_jpg)\n",
    "print(f\"Color matched image saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (50,50,3) into shape (50,36,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m             best_tile \u001b[38;5;241m=\u001b[39m resized_tiles[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# Replace the region in the mosaic with the best matching tile\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m         \u001b[43mmosaic\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m best_tile\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Save the mosaic image\u001b[39;00m\n\u001b[1;32m     62\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmosaic.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, mosaic)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (50,50,3) into shape (50,36,3)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Load the large image\n",
    "large_image = cv2.imread('large_image.jpeg')\n",
    "\n",
    "# Directory containing tile images\n",
    "tile_dir = 'tiles_images/'\n",
    "tile_images = [os.path.join(tile_dir, img) for img in os.listdir(tile_dir)]\n",
    "\n",
    "# Resize tile images to a standard size (e.g., 50x50 pixels)\n",
    "tile_size = (50, 50)\n",
    "resized_tiles = [cv2.resize(cv2.imread(img), tile_size) for img in tile_images]\n",
    "\n",
    "# Flatten and reshape tile images for k-means clustering\n",
    "reshaped_tiles = [tile.reshape(-1, 3) for tile in resized_tiles]\n",
    "\n",
    "# Number of clusters for k-means (adjust as needed)\n",
    "num_clusters = 5\n",
    "\n",
    "# Perform k-means clustering on tile colors\n",
    "kmeans = MiniBatchKMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(np.vstack(reshaped_tiles))\n",
    "\n",
    "# Iterate over each region in the large image (e.g., using a sliding window)\n",
    "window_size = tile_size  # Adjust window size as needed\n",
    "mosaic = np.zeros_like(large_image)\n",
    "\n",
    "for y in range(0, large_image.shape[0], window_size[1]):\n",
    "    for x in range(0, large_image.shape[1], window_size[0]):\n",
    "        region = large_image[y:y + window_size[1], x:x + window_size[0]]\n",
    "        region_features = region.reshape(-1, 3)\n",
    "        \n",
    "        # Predict the cluster for the region using k-means\n",
    "        cluster = kmeans.predict(region_features)\n",
    "        \n",
    "        # Compute distances between region pixels and cluster centroids\n",
    "        distances = []\n",
    "        for pixel in region_features:\n",
    "            pixel_distances = np.linalg.norm(kmeans.cluster_centers_ - pixel, axis=1)\n",
    "            distances.append(np.min(pixel_distances))\n",
    "        \n",
    "        # Find the best matching tile image based on minimum distance\n",
    "        if distances:\n",
    "            best_tile_index = np.argmin(distances)\n",
    "            if 0 <= best_tile_index < len(resized_tiles):\n",
    "                best_tile = resized_tiles[best_tile_index]\n",
    "            else:\n",
    "                # Handle index out of range by selecting a default tile (e.g., first tile)\n",
    "                best_tile = resized_tiles[0]\n",
    "        else:\n",
    "            # Handle empty distances list by selecting a default tile (e.g., first tile)\n",
    "            best_tile = resized_tiles[0]\n",
    "        \n",
    "        # Replace the region in the mosaic with the best matching tile\n",
    "        mosaic[y:y + window_size[1], x:x + window_size[0]] = best_tile\n",
    "\n",
    "\n",
    "# Save the mosaic image\n",
    "cv2.imwrite('mosaic.jpg', mosaic)\n",
    "\n",
    "# # Display the mosaic image\n",
    "# cv2.imshow('Photo Mosaic', mosaic)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def split_images(target_image_path,dir, rows=3, columns=3):\n",
    "    image_parts = []\n",
    "    target = cv2.imread(target_image_path)\n",
    "    height, width, _ = target.shape\n",
    "    part_height = height // rows\n",
    "    part_width = width // columns\n",
    "    for row in range(rows):\n",
    "        for col in range(columns):\n",
    "            start_row = row * part_height\n",
    "            end_row = (row + 1) * part_height\n",
    "            start_col = col * part_width\n",
    "            end_col = (col + 1) * part_width\n",
    "\n",
    "            part = target[start_row:end_row, start_col:end_col]\n",
    "            image_parts.append(part)\n",
    "                \n",
    "        for i, part in enumerate(image_parts):\n",
    "            cv2.imwrite(f'{dir}/part_{i+1}.jpg', part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_images('large_image.jpeg', 'output', 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Load the large image\n",
    "large_image = cv2.imread('large_image.jpeg')\n",
    "\n",
    "# Directory containing tile images\n",
    "tile_dir = 'tiles_images/'\n",
    "tile_images = [os.path.join(tile_dir, img) for img in os.listdir(tile_dir)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tiles_images/pallet3.jpeg',\n",
       " 'tiles_images/pallet2.jpeg',\n",
       " 'tiles_images/pallet5.jpeg',\n",
       " 'tiles_images/pallet4.jpeg',\n",
       " 'tiles_images/pallet8.jpeg',\n",
       " 'tiles_images/pallet7.jpeg',\n",
       " 'tiles_images/pallet6.jpeg']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize tile images to a standard size (e.g., 50x50 pixels)\n",
    "tile_size = (50, 50)\n",
    "resized_tiles = [cv2.resize(cv2.imread(img), tile_size) for img in tile_images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MiniBatchKMeans(n_clusters=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MiniBatchKMeans</label><div class=\"sk-toggleable__content\"><pre>MiniBatchKMeans(n_clusters=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MiniBatchKMeans(n_clusters=5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten and reshape tile images for k-means clustering\n",
    "reshaped_tiles = [tile.reshape(-1, 3) for tile in resized_tiles]\n",
    "\n",
    "# Number of clusters for k-means (adjust as needed)\n",
    "num_clusters = 5\n",
    "\n",
    "# Perform k-means clustering on tile colors\n",
    "kmeans = MiniBatchKMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(np.vstack(reshaped_tiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_match_and_blend(region, tile):\n",
    "    # Resize the tile to match the size of the region\n",
    "    tile_resized = cv2.resize(tile, (region.shape[1], region.shape[0]))\n",
    "    \n",
    "    # Compute the average color of the region and resized tile\n",
    "    region_avg_color = np.mean(region, axis=(0, 1))\n",
    "    tile_avg_color = np.mean(tile_resized, axis=(0, 1))\n",
    "    \n",
    "    # Calculate the color difference between region and resized tile\n",
    "    color_diff = np.linalg.norm(region_avg_color - tile_avg_color)\n",
    "    \n",
    "    # Blend the region and resized tile based on color similarity\n",
    "    alpha = 1.0 - (color_diff / 255.0)  # Adjust alpha based on color difference\n",
    "    blended_region = cv2.addWeighted(region, alpha, tile_resized, 1.0 - alpha, 0)\n",
    "    \n",
    "    return blended_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each region in the large image (e.g., using a sliding window)\n",
    "window_size = tile_size  # Adjust window size as needed\n",
    "mosaic = np.zeros_like(large_image)\n",
    "\n",
    "for y in range(0, large_image.shape[0], window_size[1]):\n",
    "    for x in range(0, large_image.shape[1], window_size[0]):\n",
    "        region = large_image[y:y + window_size[1], x:x + window_size[0]]\n",
    "        #print(region.shape)\n",
    "        region_avg_color = np.mean(region, axis=(0, 1))  # Compute average color of the region\n",
    "        \n",
    "        # Find the best matching tile image based on average color difference\n",
    "        min_diff = float('inf')\n",
    "        best_tile = None\n",
    "        for tile in resized_tiles:\n",
    "            tile_avg_color = np.mean(tile, axis=(0, 1))\n",
    "            color_diff = np.linalg.norm(region_avg_color - tile_avg_color)\n",
    "            if color_diff < min_diff:\n",
    "                min_diff = color_diff\n",
    "                best_tile = tile\n",
    "        \n",
    "        # Replace the region in the mosaic with the best matching tile\n",
    "        blended_region = color_match_and_blend(region, best_tile)    \n",
    "        mosaic[y:y + window_size[1], x:x + window_size[0]] = cv2.resize(blended_region,(region.shape[1], region.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('mosaicr4lgk.jpg', mosaic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import uuid\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "class Photomosaicv2:\n",
    "    def __init__(self, target_path, pallet_images_path, num_tiles_horizontal=5, num_tiles_vertical=5):\n",
    "        self.large_image = cv2.imread(target_path)\n",
    "        self.tile_size = self.calculate_tile_size(self.large_image, num_tiles_horizontal, num_tiles_vertical)\n",
    "        self.resized_tiles = [cv2.resize(cv2.imread(img), self.tile_size) for img in pallet_images_path]\n",
    "    \n",
    "    def calculate_tile_size(self, large_image, num_tiles_horizontal, num_tiles_vertical):\n",
    "        height, width, _ = large_image.shape\n",
    "        tile_width = width // num_tiles_horizontal\n",
    "        tile_height = height // num_tiles_vertical\n",
    "        return tile_width, tile_height\n",
    "    \n",
    "    def color_match_and_blend(self, region, tile):\n",
    "        # Resize the tile to match the size of the region\n",
    "        tile_resized = cv2.resize(tile, (region.shape[1], region.shape[0]))\n",
    "        \n",
    "        # Compute the average color of the region and resized tile\n",
    "        region_avg_color = np.mean(region, axis=(0, 1))\n",
    "        tile_avg_color = np.mean(tile_resized, axis=(0, 1))\n",
    "        \n",
    "        # Calculate the color difference between region and resized tile\n",
    "        color_diff = np.linalg.norm(region_avg_color - tile_avg_color)\n",
    "        \n",
    "        # Blend the region and resized tile based on color similarity\n",
    "        alpha = 1.0 - (color_diff / 255.0)  # Adjust alpha based on color difference\n",
    "        blended_region = cv2.addWeighted(region, alpha, tile_resized, 1.0 - alpha, 0)\n",
    "        \n",
    "        return blended_region\n",
    "\n",
    "    def resize_cover(self, img, size,interpolation=cv2.INTER_AREA):\n",
    "        h, w = img.shape[:2]\n",
    "        min_size = np.amin([h,w])\n",
    "\n",
    "        # Centralize and crop\n",
    "        crop_img = img[int(h/2-min_size/2):int(h/2+min_size/2), int(w/2-min_size/2):int(w/2+min_size/2)]\n",
    "        resized = cv2.resize(crop_img, (size[0], size[1]), interpolation=interpolation)\n",
    "        return resized #cv2.resize(img, size)\n",
    "\n",
    "    def transform(self):\n",
    "        output_image = str(uuid.uuid4())+'_matched_image.jpg'\n",
    "        window_size = self.tile_size  # Adjust window size as needed\n",
    "        mosaic = np.zeros_like(self.large_image)\n",
    "\n",
    "        for y in range(0, self.large_image.shape[0], window_size[1]):\n",
    "            for x in range(0, self.large_image.shape[1], window_size[0]):\n",
    "                region = self.large_image[y:y + window_size[1], x:x + window_size[0]]\n",
    "                #print(region.shape)\n",
    "                region_avg_color = np.mean(region, axis=(0, 1))  # Compute average color of the region\n",
    "                \n",
    "                # Find the best matching tile image based on average color difference\n",
    "                min_diff = float('inf')\n",
    "                best_tile = None\n",
    "                for tile in self.resized_tiles:\n",
    "                    tile_avg_color = np.mean(tile, axis=(0, 1))\n",
    "                    color_diff = np.linalg.norm(region_avg_color - tile_avg_color)\n",
    "                    if color_diff < min_diff:\n",
    "                        min_diff = color_diff\n",
    "                        best_tile = tile\n",
    "                \n",
    "                # Replace the region in the mosaic with the best matching tile\n",
    "                blended_region = self.color_match_and_blend(region, best_tile)    \n",
    "                mosaic[y:y + window_size[1], x:x + window_size[0]] = self.resize_cover(blended_region,(region.shape[1], region.shape[0]))\n",
    "        \n",
    "        cv2.imwrite(output_image, mosaic)\n",
    "        return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385, 686, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('large_image.jpeg')\n",
    "img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_cover(image, container_width, container_height):\n",
    "    # Calculate the aspect ratio of the image\n",
    "    aspect_ratio = image.shape[1] / image.shape[0]\n",
    "\n",
    "    # Calculate the aspect ratio of the container\n",
    "    container_aspect_ratio = container_width / container_height\n",
    "\n",
    "    if aspect_ratio > container_aspect_ratio:\n",
    "        # Resize based on width to cover the container dimensions without stretching\n",
    "        new_width = container_width\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        # Resize based on height to cover the container dimensions without stretching\n",
    "        new_height = container_height\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    # Resize the image using OpenCV's resize function with interpolation\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Calculate the starting point for cropping to cover the container dimensions\n",
    "    start_x = (new_width - container_width) // 2\n",
    "    start_y = (new_height - container_height) // 2\n",
    "\n",
    "    # Crop the resized image to cover the container dimensions\n",
    "    covered_image = resized_image[start_y:start_y + container_height, start_x:start_x + container_width]\n",
    "\n",
    "    return covered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('resizeimage.jpeg', resize_image_cover(img, 1100, 1100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25836 sha256=54538080ed127b1c2c2ca50f4c32855be3f2ce9b088314472326aecd38aa955f\n",
      "  Stored in directory: /Users/simi/Library/Caches/pip/wheels/31/d0/2c/87ce38f6052879e5b7b18f0f8b4a10ad2a9d210e908d449f16\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def resize_and_crop_image(image_path, target_height, target_width):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Calculate the aspect ratio of the image\n",
    "    aspect_ratio = image.shape[1] / image.shape[0]\n",
    "\n",
    "    # Determine the smaller dimension between target_height and target_width\n",
    "    smaller_dimension = min(target_height, target_width)\n",
    "\n",
    "    # Resize the image to match the smaller dimension while maintaining aspect ratio\n",
    "    if aspect_ratio >= 1:\n",
    "        new_width = smaller_dimension\n",
    "        new_height = int(smaller_dimension / aspect_ratio)\n",
    "    else:\n",
    "        new_width = int(smaller_dimension * aspect_ratio)\n",
    "        new_height = smaller_dimension\n",
    "\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    # Calculate the crop coordinates based on the target height and width\n",
    "    crop_x = int((new_width - target_width) / 2)\n",
    "    crop_y = int((new_height - target_height) / 2)\n",
    "\n",
    "    # Crop the image to the specified height and width\n",
    "    cropped_image = resized_image[crop_y:crop_y + target_height, crop_x:crop_x + target_width]\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('resized_imagee.jpeg', resize_and_crop_image('large_image.jpeg', 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imutils\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('large_image.jpeg')\n",
    "cv2.imwrite('resized_image.jpeg', imutils.resize(image, height=200, width=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 1100, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_resized = cv2.imread('resizeimage.jpeg')\n",
    "img_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385, 686, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('large_image.jpeg')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(input_image, crop_width, crop_height):\n",
    "    # Load the input image\n",
    "    image = cv2.imread(input_image)\n",
    "    max_resize = max(crop_height, crop_width, image.shape[0],image.shape[1])\n",
    "    if max_resize < crop_height or max_resize < crop_width:\n",
    "        max_resize = 2 * max_resize\n",
    "    image = imutils.resize(image, width=max_resize)\n",
    "\n",
    "    cv2.imwrite('resized_001.jpeg', image)\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    image_height, image_width = image.shape[:2]\n",
    "\n",
    "    # Calculate the crop region coordinates\n",
    "    start_x = (image_width - crop_width) // 2\n",
    "    end_x = start_x + crop_width\n",
    "    start_y = (image_height - crop_height) // 2\n",
    "    end_y = start_y + crop_height\n",
    "\n",
    "    # Crop the image using the calculated coordinates\n",
    "    cropped_image = image[start_y:end_y, start_x:end_x]\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('cropped.jpeg', crop_image('large_image.jpeg', 100, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
